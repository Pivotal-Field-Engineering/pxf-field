<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>PXF by Pivotal-Field-Engineering</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">      
      <script type="text/javascript" src="header.js"> </script>
      <section>
        
<h2>JSON</h2>
<h3>Overview</h3>
<p>The JSON PXF extension will read native JSON data stored in HDFS.  It supports basic projections into JSON objects.  Indexing into JSON arrays is under development.  See Installing an Extension for installation details.

<p>There are two types of ways this extension can read data.  The default expects one full JSON block per line, much like how the Twitter feed operates.  It also supports "pretty print" of JSON -- one full JSON file that contains a single root record and then an array of records that have some root identifier.  See examples below.


<h3>Required Hadoop Classpath Additions</h3>

<p><b>Note</b> the version numbers of these jar files.  Make sure you are using the proper version of the extension as well as any dependencies.</p>

<code>
# Required only for PXF with JSON<br/>
HADOOP_CLASSPATH=$HADOOP_CLASSPATH:\<br/>
$GPHD_HOME/gpxf/json-pxf-ext-&ltversion&gt.jar<br/>
</code>
<br/>
<h3>Column Specification</h3>

<p>Each column is named after the JSON object, complete with projects and array indices, if applicable.  For nested objects, used quoted column names and a period to project into the nested objects.  See below for an example.</p>

<h3>Table Specification Parameters</h3>
<table id="tfhover" class="tftable" border="1">
<tr><th>Parameter</th><th>Required?</th><th>Value</th></tr>
<tr><td>FRAGMENTER</td><td>Yes</td><td>HdfsDataFragmenter</td></tr>
<tr><td>ACCESSOR</td><td>Yes</td><td>JsonAccessor</td></tr>
<tr><td>RESOLVER</td><td>Yes</td><td>JsonResolver</td></tr>
<tr><td>ANALYZER</td><td>No</td><td>HdfsAnalyzer</td></tr>
<tr><td>IDENTIFIER</td><td>No</td><td>Use this when ONERECORDPERLINE is set to "true".  This identifies a single record of JSON</td></tr>
<tr><td>ONERECORDPERLINE</td><td>No</td><td>Default is "true",  Boolean value indicating if a single JSON record is compacted on one line</td></tr>
</table>
<br/>

<h3>One Record Per Line Example</h3>

<p>The following creates an external HAWQ table that will read data from a JSON file in HDFS table called mytestfile.json that has one record per line.</p>

<p><b>JSON Data</b></p>

<pre><code>
{"created_at":"Mon Sep 30 04:04:53 +0000 2013","id_str":"384529256681725952","text":"sigh, who knows.","source":"web","user":{"id":31424214,"location":"COLUMBUS"},"coordinates":{"type":"Point","coordinates":[-6.100,50.103]}}
{"created_at":"Mon Sep 30 04:04:54 +0000 2013","id_str":"384529260872228864","text":"I did that 12 years ago..\n\nT.T","source":"web","user":{"id":67600981,"location":"KryberWorld"},"coordinates":{"type":"Point","coordinates":[-8.100,52.104]}}
{"created_at":"Mon Sep 30 04:04:54 +0000 2013","id_str":"384529260892786688","text":"Welp guess I'll have anxiety for another week","source":"web","user":{"id":122795713,"location":"California"},"coordinates":null}
{"created_at":"Mon Sep 30 04:04:55 +0000 2013","id_str":"384529265099689984","text":"I'm craving breadsticks","source":"web","user":{"id":633364307,"location":""},"coordinates":null}
</pre></code>

<p><b>PSQL Prompt</b></p>

<p>The data read via HAWQ using a JSON extension.</p>

<pre><code>
CREATE EXTERNAL TABLE ext_json_mytestfile (
created_at TEXT,
id_str TEXT,
text TEXT,
source TEXT,
"user.id" INTEGER,
"user.location" TEXT,
"coordinates.coordinates[0]" DOUBLE PRECISION,
"coordinates.coordinates[1]" DOUBLE PRECISION
)
LOCATION('pxf://phd2:50070/user/gpadmin/mytestfile.json?FRAGMENTER=HdfsDataFragmenter&
ACCESSOR=JsonAccessor&RESOLVER=JsonResolver')
FORMAT 'CUSTOM'
(FORMATTER='gpxfwritable_import');

SELECT * FROM ext_json_mytestfile;

           created_at           |       id_str       |                     text                      | source |  user.id  | user.location | coordinates.coordinates[0] | coordinates.coordinates[1] 
--------------------------------+--------------------+-----------------------------------------------+--------+-----------+---------------+---------------------------------------------------------
 Mon Sep 30 04:04:53 +0000 2013 | 384529256681725952 | sigh, who knows.                              | web    |  31424214 | COLUMBUS      |                       -6.1 |                     50.103
 Mon Sep 30 04:04:54 +0000 2013 | 384529260872228864 | I did that 12 years ago..\n\nT.T              | web    |  67600981 | KryberWorld   |                       -8.1 |                     52.104
 Mon Sep 30 04:04:54 +0000 2013 | 384529260892786688 | Welp guess I'll have anxiety for another week | web    | 122795713 | California    |                            |                            
 Mon Sep 30 04:04:55 +0000 2013 | 384529265099689984 | I'm craving breadsticks                       | web    | 633364307 |               |                            |                            
</code></pre>

<br/>
<h3>Pretty Print Example  -- currently not working...</h3>

<p>The following creates an external HAWQ table that will read data from a JSON file in HDFS table called mypptestfile.json that has one record per line.</p>

<p><b>JSON Data</b></p>

<pre><code>
{
  "root":[
    {
      "record":{
        "created_at":"Mon Sep 30 04:04:53 +0000 2013",
        "id_str":"384529256681725952",
        "text":"sigh, who knows.",
        "source":"web",
        "user":{
          "id":31424214,
          "location":"COLUMBUS"
        },
        "coordinates":{
          "type":"Point",
          "coordinates":[
            -6.100,
            50.103
          ]
        }
      },
      "record":{
        "created_at":"Mon Sep 30 04:04:54 +0000 2013",
        "id_str":"384529260872228864",
        "text":"I did that 12 years ago..\n\nT.T",
        "source":"web",
        "user":{
          "id":67600981,
          "location":"KryberWorld"
        },
        "coordinates":{
          "type":"Point",
          "coordinates":[
            -8.100,
            52.104
          ]
        }
      },
      "record":{
        "created_at":"Mon Sep 30 04:04:54 +0000 2013",
        "id_str":"384529260892786688",
        "text":"Welp guess I'll have anxiety for another week",
        "source":"web",
        "user":{
          "id":122795713,
          "location":"California"
        },
        "coordinates":null
      },
      "record":{
        "created_at":"Mon Sep 30 04:04:55 +0000 2013",
        "id_str":"384529265099689984",
        "text":"I'm craving breadsticks",
        "source":"web",
        "user":{
          "id":633364307,
          "location":""
        },
        "coordinates":null
      }
    }
  ]
}
</code></pre>

<p><b>PSQL Prompt</b></p>

<p>The data read via HAWQ using a JSON extension.</p>

<code>
CREATE EXTERNAL TABLE ext_json_mytestfile (
created_at TEXT,
id_str TEXT,
text TEXT,
source TEXT,
"user.id" INTEGER,
"user.location" TEXT,
"coordinates.coordinates[0]" DOUBLE PRECISION,
"coordinates.coordinates[1]" DOUBLE PRECISION
)
LOCATION('pxf://phd2:50070/user/gpadmin/mypptestfile.json?FRAGMENTER=HdfsDataFragmenter&<br/>
ACCESSOR=JsonAccessor&RESOLVER=JsonResolver&<br/>
ONERECORDPERLINE=true&IDENTIFER=record')<br/>
FORMAT 'CUSTOM'<br/>
(FORMATTER='gpxfwritable_import');<br/>
<br/>
SELECT * FROM ext_json_mypptestfile;<br/>
</code>

      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/Pivotal-Field-Engineering">Pivotal-Field-Engineering</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
