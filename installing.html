<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>PXF by Pivotal-Field-Engineering</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">     
      <script type="text/javascript" src="header.js"> </script>
      <section>
        
<h2>Installing an Extension</h2>
<h3>Overview</h3>

<p>
A PXF extension is relatively simple to install.  The steps break down to:
<ol>
<li>Package extension into a JAR file</li>
<li>Put the JAR file in the /usr/lib/gphd/pxf directory on each cluster node</li>
<li>Add this JAR file name and any dependencies names to the <string></string>/etc/gphd/pxf/conf/pxf-public.classpath<strong></strong> file on each cluster node</li>
<li>Create a profile for your new extension, modifying  /etc/gphd/pxf/conf/pxf-profiles.xml</li>
<li>Restart both the HAWQ and the PXF services via Ambari. Use the Ambari UI or Ambari REST API</li>
<li>Create an external table and query data</li>
<li>If unsuccessful with classpath related errors, go back to 3 and add any missing JAR files</li>
</ol>
</p>

<h3>Example Installation</h3>

<p>
<b>Note</b> This guide was built against the Pivotal HD 3.0 release.
</p>
<p>
For example, the following steps will install the <a href="accumulo.html">Accumulo extension</a>.
</p>

<ol>
<p><li>Place the extension JAR file in gpadmin's home directory on the HAWQ master node from your local machine.  If the master node doesn't have an external address, SCP to a node that does and then the master node.</li></p>

<p><code>scp ~/accumulo-pxf-ext-&lt;version&gt;.jar gpadmin@&lt;hawq_master&gt;:~</code></p>

<p><li>As <code>gpadmin</code> on the Ambari node (from here on out), use the scp utility to copy the jar file to every HDFS and HAWQ node in the cluster (includes master and slave services).</p>

<p>
<code>
scp ./accumulo-pxf-ext-&lt;version&gt;.jar <every HDFS and HAWQ node> </every><br/>
Then on each node: mv accumulo-pxf-ext-&lt;version&gt;.jar \<br/>
/usr/lib/gphd/pxf<br/>
</code>
</li></p>

<p><li>Check out the cluster configuration using ICM.  Modify hadoop-env.sh to add the extension and any additional JAR files.  Modify pxf-profiles.xml to add a profile for your connector.</p>

<code>
icm_client fetch-configuration -l &lt;clustername&gt; -o &lt;configdir&gt;<br/>
<br/>
vi &lt;configdir&gt;/hdfs/hadoop-env.sh<br/>
<br/>
# Required only for PXF with Accumulo<br/>
HADOOP_CLASSPATH=$HADOOP_CLASSPATH:\<br/>
$GPHD_HOME/pxf/accumulo-pxf-ext-&lt;version&gt;.jar:\<br/>
$GPHD_HOME/accumulo/lib/accumulo-core.jar:\<br/>
$GPHD_HOME/accumulo/lib/accumulo-core.jar:\<br/>
$GPHD_HOME/accumulo/lib/accumulo-fate.jar:\<br/>
$GPHD_HOME/accumulo/lib/accumulo-minicluster.jar:\<br/>
$GPHD_HOME/accumulo/lib/accumulo-proxy.jar:\<br/>
$GPHD_HOME/accumulo/lib/accumulo-server.jar:\<br/>
$GPHD_HOME/accumulo/lib/accumulo-start.jar:\<br/>
$GPHD_HOME/accumulo/lib/accumulo-test.jar:\<br/>
$GPHD_HOME/accumulo/lib/accumulo-trace.jar:\<br/>
$GPHD_HOME/accumulo/lib/libthrift.jar:\<br/>
$GPHD_HOME/accumulo/lib/jline.jar:\<br/>
$GPHD_HOME/accumulo/conf<br/>
<br/>
vi &lt;configdir&gt;/gpxf/pxf-profiles.xml<br/>
&lt;profile&gt;<br\><br/>
&nbsp;&nbsp;&lt;name&gt;Accumulo&lt;/name&gt;<br\><br/>
&nbsp;&nbsp;&lt;description&gt;A profile for data stored in Accumulo. Must specify INSTANCE, QUORUM, USER, PASSWORD, and scan AUTHS in DDL.<br\><br/>
&nbsp;&nbsp;&lt;/description&gt;<br\><br/>
&nbsp;&nbsp;&lt;plugins&gt;<br\><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;fragmenter&gt;com.pivotal.pxf.plugins.accumulo.AccumuloFragmenter&lt;/fragmenter&gt;<br\><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;accessor&gt;com.pivotal.pxf.plugins.accumulo.AccumuloAccessor&lt;/accessor&gt;<br\><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;resolver&gt;com.pivotal.pxf.plugins.accumulo.AccumuloResolver&lt;/resolver&gt;<br\><br/>
&nbsp;&nbsp;&lt;/plugins&gt;<br\><br/>
&lt;/profile&gt;<br\><br/>
</code></li>

<p><li>Reconfigure the cluster using ICM.</p>

<p>
<code>
icm_client stop -l &lt;clustername&gt;<br/>
icm_client reconfigure -l &lt;clustername&gt; -c &lt;configdir&gt; -s -p<br/>
icm_client start -l &lt;clustername&gt;<br/>
</code>
</p></li>
</ol>

<h3>Troubleshooting</h3>

<p>After creating an external table, the most common error when creating and querying an external table using your extension is a CLASSPATH issue -- Class not found, no such method, etc.  You'll need to add the missing JAR files to the HADOOP_CLASSPATH.  Note that, as in the Accumulo extension, libthrift and jline are outside of the Accumulo-related JAR files but are added the Hadoop classpath here.  This is because the versions that come packaged with Hadoop are not up to date with Accumulo and will not work until removed from the /usr/lib/gphd/hadoop/lib directory and the Accumulo ones are added.  This can cause all sorts of problems... so be careful.  The fun is in the danger!</p>

      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/Pivotal-Field-Engineering">Pivotal-Field-Engineering</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
